# -*- coding: utf-8 -*-
"""HW2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/omid-reza/COMP-6781/blob/main/Assignment2/HW2.ipynb
"""

#!pip install datasets
#!gdown https://drive.google.com/file/d/1-mB6idLW5Jg4aE68jOj5NDcDxRNlMXpu/view?usp=sharing --fuzzy

import torch
torch.manual_seed(0)

from datasets import load_dataset
from random import sample

wikitext = load_dataset("wikipedia", "20220301.simple")
trim_dataset = sample(wikitext['train']['text'], 5000)

trim_dataset[0]

import nltk
nltk.download('stopwords')

import string
import re
from nltk.corpus import stopwords

def preprocess_data(data):
    """ Method to clean text from noise and standarize text across the different classes.
        The preprocessing includes converting to joining all datapoints, lowercase, removing punctuation, and removing stopwords.
    Arguments
    ---------
    text : List of String
       Text to clean
    Returns
    -------
    text : String
        Cleaned and joined text
    """

    stop_words = set(stopwords.words('english'))

    text = ' '.join(data)  # join all text in one single string
    text = text.lower()  # make everything lower case
    text = re.sub(r'\n', ' ', text)  # remove \n characters
    text = re.sub(r'references', '', text)  # remove word "References"
    text = re.sub(r'[^\w\s]', '', text)  # remove any punctuation or special characters
    text = re.sub(r'\d+', '', text)  # remove all numbers
    text = ' '.join([word for word in text.split() if word not in stop_words])  # remove all stopwords

    return text

# Preprocess the data
text = preprocess_data(trim_dataset)

def vocab_frequency(text):
    """ Creates dictionary of frequencies based on a dataset.
    Arguments
    ---------
    text : string
        Preprocessed text
    Returns
    -------
    vocab_dict : dictionary
        Dictionary of words and their frequencies with the format {word: frequency}
    """
    vocab_dict = {}

    # Split the text into words
    words = text.split()

    # Count the frequency of each word
    for word in words:
        vocab_dict[word] = vocab_dict.get(word, 0) + 1
    return vocab_dict

# Create the vocabulary
vocabulary = vocab_frequency(text)

len(vocabulary)

import torch
import torch.nn as nn

def word_to_index(vocabulary):
    """ Method to create vocabulary to index mapping.
    Arguments
    ---------
    vocabulary : Dictionary
       Dictionary of format {word:frequency}
    Returns
    -------
    word_to_index : Dictionary
        Dictionary mapping words to index with format {word:index}
    """
    word_to_index = {'OOV': 0}  # Initialize with 'OOV' at index 0

    # Assign indices to words in the vocabulary
    for index, word in enumerate(vocabulary.keys(), start=1):  # Start from 1 since 0 is reserved for 'OOV'
        word_to_index[word] = index

    return word_to_index

# Create the word_to_index mapping
word_to_index = word_to_index(vocabulary)

def generate_dataset(data, window_size, word_to_index):
    """ Method to generate training dataset for CBOW.
    Arguments
    ---------
    data : String
       Training dataset
    window_size : int
       Size of the context window
    word_to_index : Dictionary
       Dictionary mapping words to index with format {word:index}
    Returns
    -------
    surroundings : N x W Tensor
        Tensor with index of surrounding words, with N being the number of samples and W being the window size
    targets : Tensor
        Tensor with index of target word
    """
    surroundings = []
    targets = []
    data = data.split()

    for i in range(window_size, len(data) - window_size):
        # Get surrounding words based on window size
        surrounding = data[i-window_size:i] + data[i+1:i+window_size+1]

        # Get target word (middle word)
        target = data[i]

        # Convert words to indices, using 'OOV' for unknown words
        surrounding_indices = [word_to_index.get(word, word_to_index['OOV']) for word in surrounding]
        target_index = word_to_index.get(target, word_to_index['OOV'])

        # Append to surroundings and targets
        surroundings.append(surrounding_indices)
        targets.append(target_index)

    # Convert lists to tensors
    surroundings = torch.tensor(surroundings, dtype=torch.long)
    targets = torch.tensor(targets, dtype=torch.long)

    return surroundings, targets

# Generate the dataset
t_surroundings, t_targets = generate_dataset(text, 2, word_to_index)

class CBOW(nn.Module):
    def __init__(self, vocab_size, embed_dim=300):
        """ Class to define the CBOW model
        Attributes
        ---------
        vocab_size : int
            Size of the vocabulary
        embed_dim : int
            Size of the embedding layer
        """
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.linear = nn.Linear(embed_dim, vocab_size)

    def forward(self, x):
        # Pass input through embedding layer
        emb = self.embedding(x)

        # Average and resize (size must be batch_size x embed_dim)
        average = torch.mean(emb, dim=1)

        # Pass through linear layer
        out = self.linear(average)

        return out

from torch.utils.data import DataLoader
#creation of dataloader for training
train_dataloader=DataLoader(list(zip(t_surroundings,t_targets)),batch_size=64,shuffle=True) #Here please change batch size depending of your GPU capacities (if GPU runs out of memory lower batch_size)

from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CBOW(len(word_to_index)).to(device)
loss_function = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
epochs = 50

# Training loop
for epoch in range(epochs):
    total_loss = 0
    for surr, tar in tqdm(train_dataloader):
        surr, tar = surr.to(device), tar.to(device)
        output = model(surr)
        loss = loss_function(output, tar)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    # Print epoch loss
    print(f"Epoch {epoch + 1} loss: {total_loss / len(train_dataloader)}")

def get_embedding(word, model, word_to_index):
    """ Method to get the embedding vector for a given word.
    Arguments
    ---------
    word : String
       Word given
    model : nn.Module
       CBOW model
    word_to_index : Dictionary
       Dictionary mapping words to index with format {word:index}
    Returns
    -------
    word_embedding : Tensor
        Embedding vector for the given word
    """
    # Get word index
    index = word_to_index.get(word, word_to_index['OOV'])

    with torch.no_grad():
        # Get the weights of the embedding layer
        embedding_weights = model.embedding.weight
        embedding_weights.requires_grad = False
        # Extract the embedding vector for the given word index
        word_embedding = embedding_weights[index]
    return word_embedding

# Test the function by getting embedding of the word "shot"
shot_embedding = get_embedding("shot", model, word_to_index)
print("Embedding for 'shot':")
print(shot_embedding)
print("\nEmbedding shape:", shot_embedding.shape)

from torch.nn.functional import cosine_similarity as cosine_similarity_pt
def cosine_similarity(v1, v2):
    """ Method to calculate cosine similarity between two vectors.
    Arguments
    ---------
    v1 : Tensor
       First vector
    v2 : Tensor
       Second vector
    Returns
    -------
    cosine_similarity : float
        Cosine similarity between v1 and v2
    """
    dot_product = torch.dot(v1, v2)
    magnitude_v1 = torch.norm(v1)
    magnitude_v2 = torch.norm(v2)
    if magnitude_v1.item() == 0 or magnitude_v2.item() == 0:
        return 0.0
    cosine_similarity = dot_product / (magnitude_v1 * magnitude_v2)
    return cosine_similarity

def get_k_nearest_words(k, word, vocabulary, model, word_to_index):
    """ Method to find the k nearest words of a given vector
    Arguments
    ---------
    k : int
       Number of nearest words to return
    word : str or torch.Tensor
       Word or embedding vector for the given word
    vocabulary : Dictionary
       Dictionary mapping words to frequency with format {word:frequency}
    model : nn.Module
       CBOW model
    word_to_index : Dictionary
       Dictionary mapping words to index with format {word:index}
    Returns
    -------
    similar : List of Strings
        List of k nearest words to the given word
    """
    similarity_scores = torch.zeros(len(vocabulary))

    # Check if the input is a word (string) or an embedding (tensor)
    if isinstance(word, torch.Tensor):
        word_embedding = word
    else:
        word_embedding = get_embedding(word, model, word_to_index)

    # Fill similarity scores matrix using the word and our cosine_similarity function
    for i, (w, _) in enumerate(vocabulary.items()):
        current_word_embedding = get_embedding(w, model, word_to_index)
        similarity_scores[i] = cosine_similarity(word_embedding, current_word_embedding)

    # Get the k highest similarity scores
    k_first = torch.topk(similarity_scores, k)

    # Create a list of the k nearest words
    similar = [list(vocabulary.keys())[i] for i in k_first.indices]

    return similar

import pandas as pd

def test_analogy(model, word_to_index, analogy_file):
    """ Method to test accuracy of CBOW embeddings on analogy tasks.
    Arguments
    ---------
    model : nn.Module
       CBOW model
    word_to_index : Dictionary
       Dictionary mapping words to index with format {word:index}
    analogy_file : String
       File containing analogy tasks
    Returns
    -------
    accuracy : float
        accuracy of the model on the analogy tasks
    """
    df = pd.read_csv(analogy_file)
    df = df[df.category=='capital-common-countries']  # using capital cities subset of test set
    correct = 0
    total = 0

    for index, row in df.iterrows():
        # Extract words and standardize to lowercase
        word_one = row['word_one'].lower()
        word_two = row['word_two'].lower()
        word_three = row['word_three'].lower()
        word_four = row['word_four'].lower()

        try:
            # Get embeddings of all words
            embedding_one = get_embedding(word_one, model, word_to_index)
            embedding_two = get_embedding(word_two, model, word_to_index)
            embedding_three = get_embedding(word_three, model, word_to_index)
            embedding_four = get_embedding(word_four, model, word_to_index)

            result = embedding_two - embedding_one + embedding_three
            prediction = get_k_nearest_words(10, result, vocabulary, model, word_to_index)
            is_correct = word_four in prediction
            print(f"Test Analogy: {word_one} => {word_two} || {word_three} => {word_four} || Prediction: {prediction}, Ground Truth: {is_correct}")
            # Check if word_four is in prediction
            if is_correct:
                correct += 1
            total += 1
        except KeyError:
            # Skip this analogy if any word is not in the vocabulary
            continue

    if total != 0:
        accuracy = correct / total
    else:
        return 'No word was found in the embeddings'
    return accuracy

# Test the analogy function
accuracy = test_analogy(model, word_to_index, 'TestSet_sample.csv')
print(f"Accuracy on analogy tasks: {accuracy}")

import numpy as np
import pandas as pd
import torch
import sys

from sklearn.manifold import TSNE
import plotly.graph_objects as go

# Assuming the model is already defined and trained
model.eval()

# Get embeddings for the first 1000 words
words = list(word_to_index.keys())[:1000]
embeddings = model.embedding.weight.detach().cpu().numpy()[:1000]

# Create TSNE
tsne = TSNE(n_components=2, random_state=0)
tsne_results = tsne.fit_transform(embeddings)

# Create a DataFrame for easier plotting
df = pd.DataFrame({
    'x': tsne_results[:, 0],
    'y': tsne_results[:, 1],
    'word': words
})

# Create the plot
fig = go.Figure()

fig.add_trace(go.Scatter(
    x=df['x'],
    y=df['y'],
    mode='markers+text',
    text=df['word'],
    textposition="top center",
    hoverinfo='text',
    marker=dict(size=5, color=df['x'], colorscale='Viridis', showscale=True)
))

fig.update_layout(
    title='t-SNE visualization of word embeddings',
    xaxis_title='t-SNE dimension 1',
    yaxis_title='t-SNE dimension 2',
    width=1000,
    height=800
)

# fig.show()
fig.write_image("t-SNE_word_embeddings_all_5k_50epc_.png")
